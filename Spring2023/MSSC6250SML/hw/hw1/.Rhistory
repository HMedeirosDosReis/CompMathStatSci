num_iters <- 15000
results <- gradDescent(new_X, y, theta, alpha, num_iters)
theta <- results[[1]]
print(theta)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
m <- 250
new_X<-cbind(rep(1, 100), x)
theta<-rep(0,5)
compCost<-function(X, y, theta){
m <- length(y)
J <- sum((X%*%theta- y)^2)/(2*m)
return(J)
}
gradDescent<-function(X, y, Beta, alpha, num_iters){
for(i in 1:num_iters)
{
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
}
# for a R function to return two values, we need to use a list to store them:
results<-list(Beta)
return(results)
}
alpha <- 1
num_iters <- 15000
results <- gradDescent(new_X, y, theta, alpha, num_iters)
theta <- results[[1]]
print(theta)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
m <- 250
new_X<-cbind(rep(1, 100), x)
theta<-rep(0,5)
compCost<-function(X, y, theta){
m <- length(y)
J <- sum((X%*%theta- y)^2)/(2*m)
return(J)
}
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
}
# for a R function to return two values, we need to use a list to store them:
results<-list(Beta)
return(results)
}
alpha <- 1
num_iters <- 15000
results <- gradDescent(new_X, y, theta, alpha)
theta <- results[[1]]
print(theta)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
m <- 250
new_X<-cbind(rep(1, 100), x)
theta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
}
# for a R function to return two values, we need to use a list to store them:
results<-list(Beta)
return(results)
}
alpha <- 1
num_iters <- 15000
results <- gradDescent(new_X, y, theta, alpha)
theta <- results[[1]]
print(theta)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
m <- 250
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
}
# for a R function to return two values, we need to use a list to store them:
results<-list(Beta)
return(results)
}
alpha <- 1
num_iters <- 15000
results <- gradDescent(new_X, y, Beta, alpha)
Beta <- results[[1]]
print(Beta)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
m <- 250
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
}
# for a R function to return two values, we need to use a list to store them:
results<-Beta
return(results)
}
alpha <- 1
num_iters <- 15000
results <- gradDescent(new_X, y, Beta, alpha)
Beta <- results[[1]]
print(Beta)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
m <- 250
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
}
# for a R function to return two values, we need to use a list to store them:
return(Beta)
}
alpha <- 1
num_iters <- 15000
results <- gradDescent(new_X, y, Beta, alpha)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
}
# for a R function to return two values, we need to use a list to store them:
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(mean(Beta_prev)-mean(Beta)<=0.0001)
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(Beta_prev-Beta<=0.0001)
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.1))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
clear()
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0000001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0000000000001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:50)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0000000000001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0000000000001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
gradDescent<-function(X, y, Beta, alpha){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + alpha*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
gradDescent<-function(X, y, Beta, step_size){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + step_size*(1/250)*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
size(y)
length(y)
gradDescent<-function(X, y, Beta, step_size){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + step_size*(1/length(y))*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
step_size <- 1
results <- gradDescent(new_X, y, Beta, step_size)
print(results)
optimal <- lm(y~x)
print(coefficients(optimal))
gradDescent<-function(X, y, Beta, step_size){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + step_size*(1/length(y))*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
step_size <- 1
Betas <- gradDescent(new_X, y, Beta, step_size)
#compare results
print(Betas)
optimal <- lm(y~x)
print(coefficients(optimal))
gradient_Descent<-function(X, y, Beta, step_size){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + step_size*(1/length(y))*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
step_size <- 1
Betas <- gradient_Descent(new_X, y, Beta, step_size)
#compare results
print(Betas)
optimal <- lm(y~x)
print(coefficients(optimal))
gradient_Descent<-function(X, y, Beta, step_size){
for(i in 1:5000)
{
Beta_prev <- Beta
Beta<- Beta + step_size*(1/length(y))*(t(X)%*%(y-X%*%Beta))
if(all(abs(Beta_prev-Beta) < 0.0001))
{
print("converged")
return(Beta)
}
}
return(Beta)
}
x <- matrix(rnorm(1000), ncol = 4)
y <-  10*x[,1]^4+2*x[,2]^3+4*x[,3]^2+3*x[,4]
new_X<-cbind(rep(1, 100), x)
Beta<-rep(0,5)
step_size <- 1
Betas <- gradient_Descent(new_X, y, Beta, step_size)
#compare results
print(Betas)
optimal <- lm(y~x)
print(coefficients(optimal))
